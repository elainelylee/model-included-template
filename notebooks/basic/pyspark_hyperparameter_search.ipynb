{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081004b-7560-459a-9905-b32a89074e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/mnt/code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19cf789-ed97-4e65-9489-1c782b1709a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this code is generated by the Domino Code Assist toolbar button\n",
    "import domino_code_assist as dca\n",
    "dca.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fcfe04-ed5b-4c75-9494-4d3239149086",
   "metadata": {},
   "source": [
    "## Spark and Mlflow\n",
    "\n",
    "This is an example of using Spark with Mlflow. It currently *(upto 5.7) only works for on-demand Spark*.\n",
    "\n",
    "**Starting 5.8, it will work for external spark as well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b9589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d51bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe23fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.entities import Param,Metric,RunTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736b7cd5-4cc8-4669-b574-9f8715ef2ccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = 'SPARK'+'-' + os.environ['DOMINO_STARTING_USERNAME'] + '-' + os.environ['DOMINO_PROJECT_NAME']\n",
    "model_name = 'SPARK'+'-' + os.environ['DOMINO_PROJECT_NAME']\n",
    "print(\"experiment_name:\", experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment_id = client.get_experiment_by_name(experiment_name).experiment_id\n",
    "print(\"experiment_id:\", experiment_id)\n",
    "\n",
    "import time\n",
    "now = round(time.time())\n",
    "time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(now))\n",
    "tags={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0cc447",
   "metadata": {},
   "source": [
    "# HELLO WORLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ba355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(alpha, run_origin, tags={}):\n",
    "    with mlflow.start_run(run_name=run_origin,tags=tags) as run:\n",
    "        print(\"runId:\", run.info.run_uuid)\n",
    "        print(\"artifact_uri:\", mlflow.get_artifact_uri())\n",
    "        print(\"alpha:\", alpha)\n",
    "        print(\"run_origin:\", run_origin)\n",
    "        mlflow.log_param(\"alpha\", alpha)\n",
    "        mlflow.log_metric(\"rmse\", 0.789)\n",
    "        mlflow.set_tag(\"run_origin\", run_origin)\n",
    "        with open(\"info.txt\", \"w\") as f:\n",
    "            f.write(\"Hi artifact\")\n",
    "        mlflow.log_artifact(\"info.txt\")\n",
    "        params = [ Param(\"p1\",\"0.1\"), Param(\"p2\",\"0.2\") ]\n",
    "        metrics = [ Metric(\"m1\",0.1,now,0), Metric(\"m2\",0.2,now,0) ]\n",
    "        tags = [ RunTag(\"t1\",\"hi1\"), RunTag(\"t2\",\"hi2\") ]\n",
    "        client.log_batch(run.info.run_uuid, metrics, params, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad598c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "run(\"0.1\", \"JupyterLab\", tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010bccc",
   "metadata": {},
   "source": [
    "# MLflow for Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce59a06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow.spark\n",
    "import os\n",
    "import shutil\n",
    "#from pyspark.sql import SparkSession\n",
    "# Create and persist some dummy data\n",
    "# Note: On environments like Databricks with pre-created SparkSessions,\n",
    "# ensure the org.mlflow:mlflow-spark:1.11.0 is attached as a library to\n",
    "# your cluster\n",
    "spark = (SparkSession.builder\n",
    "            .config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:1.11.0\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate())\n",
    "df = spark.createDataFrame([\n",
    "        (4, \"spark i j k\"),\n",
    "        (5, \"l m n\"),\n",
    "        (6, \"spark hadoop spark\"),\n",
    "        (7, \"apache hadoop\")], [\"id\", \"text\"])\n",
    "import tempfile\n",
    "tempdir = tempfile.mkdtemp()\n",
    "df.write.csv(os.path.join(tempdir, \"my-data-path\"), header=True)\n",
    "\n",
    "# Enable Spark datasource autologging.\n",
    "mlflow.spark.autolog()\n",
    "loaded_df = spark.read.csv(os.path.join(tempdir, \"my-data-path\"),\n",
    "                header=True, inferSchema=True)\n",
    "# Call toPandas() to trigger a read of the Spark datasource. Datasource info\n",
    "# (path and format) is logged to the current active run, or the\n",
    "# next-created MLflow run if no run is currently active\n",
    "with mlflow.start_run(run_name='spark.autolog sample', tags=tags) as active_run:\n",
    "    pandas_df = loaded_df.toPandas()\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc03c1",
   "metadata": {},
   "source": [
    "# PySpark Hyperparameter Tuning - Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10ad89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "            .config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:1.11.0\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate())\n",
    "\n",
    "mlflow.pyspark.ml.autolog(log_models=True, disable=False, exclusive=False, disable_for_unsupported_versions=False, silent=False, log_post_training_metrics=True)\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "mlflow.start_run(run_name='cross-validation', tags=tags)\n",
    "\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)\n",
    "\n",
    "mlflow.end_run()    \n",
    "    \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b06071",
   "metadata": {},
   "source": [
    "# PySpark Hyperparameter Tuning - Train-Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d6bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "            .config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:1.11.0\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate())\n",
    "\n",
    "mlflow.pyspark.ml.autolog(log_models=True, disable=False, exclusive=False, disable_for_unsupported_versions=False, silent=False, log_post_training_metrics=True)\n",
    "\n",
    "mlflow.start_run(run_name='train-validation split', tags=tags)\n",
    "\n",
    "# Prepare training and test data.\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"/mnt/code/data/spark/data/mllib/sample_linear_regression_data.txt\")\n",
    "train, test = data.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "lr = LinearRegression(maxIter=10)\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "model.transform(test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show()\n",
    "\n",
    "mlflow.end_run()  \n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214afdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Call it to end an active run if there are exceptions\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cc1b5b-250f-4cec-8d8f-2fe32633696a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27b920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fc40a-0c8c-426c-a7cb-21525d7e3fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
